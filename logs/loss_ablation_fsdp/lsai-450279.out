START TIME: Mon May 19 16:10:28 CEST 2025
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
2025-05-19 16:10:49,008 - root - INFO - Setting seed to 42
2025-05-19 16:10:49,008 - root - INFO - Setting seed to 42
2025-05-19 16:10:49,008 - root - INFO - Setting seed to 42
2025-05-19 16:10:49,008 - root - INFO - Setting seed to 42
2025-05-19 16:10:49,008 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, scale=1, set_seed=42)
2025-05-19 16:10:55,724 - root - INFO - [rank 0] world size: 4
2025-05-19 16:10:55,724 - root - INFO - Setting up DataLoaders...
2025-05-19 16:10:58,582 - root - INFO - Setting up Model...
Total params: 8053329920
2025-05-19 16:11:33,144 - root - INFO - [rank 3] model is now: FullyShardedDataParallel
2025-05-19 16:11:33,144 - root - INFO - [rank 3] local params: 2013332480
Total params: 8053329920
2025-05-19 16:11:33,413 - root - INFO - [rank 0] model is now: FullyShardedDataParallel
2025-05-19 16:11:33,414 - root - INFO - [rank 0] local params: 2013332480
2025-05-19 16:11:33,415 - root - INFO - Starting training!
Total params: 8053329920
2025-05-19 16:11:35,032 - root - INFO - [rank 1] model is now: FullyShardedDataParallel
2025-05-19 16:11:35,033 - root - INFO - [rank 1] local params: 2013332480
Total params: 8053329920
2025-05-19 16:11:35,574 - root - INFO - [rank 2] model is now: FullyShardedDataParallel
2025-05-19 16:11:35,575 - root - INFO - [rank 2] local params: 2013332480
2025-05-19 16:11:43,295 - root - INFO - Step: 1 | Loss: 11.91 | Tokens per second: 414.91 | Training tokens per second (%): 19.38 | MFU (%): 0.78 | TFLOPs: 7.69
2025-05-19 16:11:45,535 - root - INFO - Step: 5 | Loss: 11.92 | Tokens per second: 7384.53 | Training tokens per second (%): 11.41 | MFU (%): 13.83 | TFLOPs: 136.78
2025-05-19 16:11:48,417 - root - INFO - Step: 10 | Loss: 11.89 | Tokens per second: 7158.11 | Training tokens per second (%): 25.72 | MFU (%): 13.41 | TFLOPs: 132.59
2025-05-19 16:11:51,302 - root - INFO - Step: 15 | Loss: 11.67 | Tokens per second: 7149.92 | Training tokens per second (%): 35.21 | MFU (%): 13.39 | TFLOPs: 132.43
2025-05-19 16:11:54,190 - root - INFO - Step: 20 | Loss: 11.32 | Tokens per second: 7144.36 | Training tokens per second (%): 34.78 | MFU (%): 13.38 | TFLOPs: 132.33
2025-05-19 16:11:57,020 - root - INFO - Step: 25 | Loss: 10.83 | Tokens per second: 7290.74 | Training tokens per second (%): 18.28 | MFU (%): 13.65 | TFLOPs: 135.04
2025-05-19 16:11:59,888 - root - INFO - Step: 30 | Loss: 9.78 | Tokens per second: 7192.40 | Training tokens per second (%): 26.99 | MFU (%): 13.47 | TFLOPs: 133.22
2025-05-19 16:12:02,696 - root - INFO - Step: 35 | Loss: 9.88 | Tokens per second: 7349.77 | Training tokens per second (%): 13.78 | MFU (%): 13.76 | TFLOPs: 136.14
2025-05-19 16:12:05,498 - root - INFO - Step: 40 | Loss: 9.86 | Tokens per second: 7363.67 | Training tokens per second (%): 9.95 | MFU (%): 13.79 | TFLOPs: 136.39
2025-05-19 16:12:08,324 - root - INFO - Step: 45 | Loss: 9.35 | Tokens per second: 7302.12 | Training tokens per second (%): 15.59 | MFU (%): 13.68 | TFLOPs: 135.25
2025-05-19 16:12:11,135 - root - INFO - Step: 50 | Loss: 9.22 | Tokens per second: 7338.05 | Training tokens per second (%): 10.93 | MFU (%): 13.74 | TFLOPs: 135.92
2025-05-19 16:12:14,009 - root - INFO - Step: 55 | Loss: 9.34 | Tokens per second: 7179.82 | Training tokens per second (%): 28.32 | MFU (%): 13.45 | TFLOPs: 132.99
2025-05-19 16:12:16,867 - root - INFO - Step: 60 | Loss: 8.70 | Tokens per second: 7217.29 | Training tokens per second (%): 26.71 | MFU (%): 13.52 | TFLOPs: 133.68
2025-05-19 16:12:19,725 - root - INFO - Step: 65 | Loss: 8.95 | Tokens per second: 7220.08 | Training tokens per second (%): 24.18 | MFU (%): 13.52 | TFLOPs: 133.73
2025-05-19 16:12:22,676 - root - INFO - Step: 70 | Loss: 8.24 | Tokens per second: 6988.88 | Training tokens per second (%): 26.25 | MFU (%): 13.09 | TFLOPs: 129.45
2025-05-19 16:12:25,506 - root - INFO - Step: 75 | Loss: 8.34 | Tokens per second: 7290.33 | Training tokens per second (%): 16.89 | MFU (%): 13.65 | TFLOPs: 135.03
2025-05-19 16:12:28,338 - root - INFO - Step: 80 | Loss: 8.24 | Tokens per second: 7286.95 | Training tokens per second (%): 17.36 | MFU (%): 13.65 | TFLOPs: 134.97
2025-05-19 16:12:31,157 - root - INFO - Step: 85 | Loss: 8.34 | Tokens per second: 7317.27 | Training tokens per second (%): 16.04 | MFU (%): 13.70 | TFLOPs: 135.53
2025-05-19 16:12:34,140 - root - INFO - Step: 90 | Loss: 7.80 | Tokens per second: 6914.02 | Training tokens per second (%): 57.98 | MFU (%): 12.95 | TFLOPs: 128.06
2025-05-19 16:12:37,114 - root - INFO - Step: 95 | Loss: 7.53 | Tokens per second: 6935.40 | Training tokens per second (%): 57.90 | MFU (%): 12.99 | TFLOPs: 128.46
2025-05-19 16:12:40,217 - root - INFO - Step: 100 | Loss: 7.48 | Tokens per second: 6646.65 | Training tokens per second (%): 93.89 | MFU (%): 12.45 | TFLOPs: 123.11
2025-05-19 16:12:40,217 - root - INFO - Training completed
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 922, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    if result.is_failed():
       ^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'is_failed'
[W519 16:12:42.336829397 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006442]:42238, remote=[nid006442]:12355): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x4000561abdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40000ca089c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x40000ca08fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x40000ca0d124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x148 (0x40000ca0ec48 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2c (0x40000ca0f10c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x9c (0x40000ca1061c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xfd9044 (0x400007039044 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x63c060 (0x40000669c060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: /usr/bin/python3() [0x503e14]
frame #10: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python3)
frame #11: /usr/bin/python3() [0x4c709c]
frame #12: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #13: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python3)
frame #14: /usr/bin/python3() [0x529450]
frame #15: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python3)
frame #16: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #17: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python3)
frame #18: /usr/bin/python3() [0x55f968]
frame #19: /usr/bin/python3() [0x503c0c]
frame #20: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python3)
frame #21: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python3)
frame #22: /usr/bin/python3() [0x68bad8]
frame #23: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python3)
frame #24: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python3)
frame #25: <unknown function> + 0x284c4 (0x4000055384c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: __libc_start_main + 0x98 (0x400005538598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #27: _start + 0x30 (0x5f6e30 in /usr/bin/python3)

W0519 16:12:42.323000 164252 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006442_164252_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
[W519 16:12:42.349647087 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006442]:42238, remote=[nid006442]:12355): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x4000561abdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40000ca089c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x40000ca08fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x40000ca0d124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x148 (0x40000ca0ec48 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2c (0x40000ca0f10c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x9c (0x40000ca1061c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xfd9044 (0x400007039044 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x63c060 (0x40000669c060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: /usr/bin/python3() [0x503e14]
frame #10: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python3)
frame #11: /usr/bin/python3() [0x4c709c]
frame #12: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #13: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python3)
frame #14: /usr/bin/python3() [0x529450]
frame #15: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python3)
frame #16: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #17: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python3)
frame #18: /usr/bin/python3() [0x55f968]
frame #19: /usr/bin/python3() [0x503c0c]
frame #20: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python3)
frame #21: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python3)
frame #22: /usr/bin/python3() [0x68bad8]
frame #23: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python3)
frame #24: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python3)
frame #25: <unknown function> + 0x284c4 (0x4000055384c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: __libc_start_main + 0x98 (0x400005538598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #27: _start + 0x30 (0x5f6e30 in /usr/bin/python3)

W0519 16:12:42.335000 164252 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006442_164252_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 117, in _call_store
    return getattr(self._store, store_op)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistNetworkError: failed to recv, got 0 bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 922, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 683, in _initialize_workers
    self._rendezvous(worker_group)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 500, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 1162, in next_rendezvous
    self._op_executor.run(join_op, deadline, self._get_deadline)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 648, in run
    has_set = self._state_holder.sync()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 437, in sync
    get_response = self._backend.get_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 75, in get_state
    base64_state: bytes = self._call_store("get", self._key)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 119, in _call_store
    raise RendezvousConnectionError(
torch.distributed.elastic.rendezvous.api.RendezvousConnectionError: The connection to the C10d store has failed. See inner exception for details.
[W519 16:12:42.434623107 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006442]:42230, remote=[nid006442]:12355): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x40005986bdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x4000100c89c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x4000100c8fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x4000100cd124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x148 (0x4000100cec48 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2c (0x4000100cf10c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x9c (0x4000100d061c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xfd9044 (0x40000a6f9044 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x63c060 (0x400009d5c060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: /usr/bin/python3() [0x503e14]
frame #10: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python3)
frame #11: /usr/bin/python3() [0x4c709c]
frame #12: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #13: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python3)
frame #14: /usr/bin/python3() [0x529450]
frame #15: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python3)
frame #16: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #17: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python3)
frame #18: /usr/bin/python3() [0x55f968]
frame #19: /usr/bin/python3() [0x503c0c]
frame #20: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python3)
frame #21: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python3)
frame #22: /usr/bin/python3() [0x68bad8]
frame #23: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python3)
frame #24: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python3)
frame #25: <unknown function> + 0x284c4 (0x400008bf84c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: __libc_start_main + 0x98 (0x400008bf8598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #27: _start + 0x30 (0x5f6e30 in /usr/bin/python3)

W0519 16:12:42.420000 164254 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006442_164254_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
[W519 16:12:42.446222438 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006442]:42230, remote=[nid006442]:12355): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x40005986bdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x4000100c89c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x4000100c8fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x4000100cd124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x148 (0x4000100cec48 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2c (0x4000100cf10c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x9c (0x4000100d061c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xfd9044 (0x40000a6f9044 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x63c060 (0x400009d5c060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: /usr/bin/python3() [0x503e14]
frame #10: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python3)
frame #11: /usr/bin/python3() [0x4c709c]
frame #12: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #13: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python3)
frame #14: /usr/bin/python3() [0x529450]
frame #15: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python3)
frame #16: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #17: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python3)
frame #18: /usr/bin/python3() [0x55f968]
frame #19: /usr/bin/python3() [0x503c0c]
frame #20: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python3)
frame #21: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python3)
frame #22: /usr/bin/python3() [0x68bad8]
frame #23: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python3)
frame #24: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python3)
frame #25: <unknown function> + 0x284c4 (0x400008bf84c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: __libc_start_main + 0x98 (0x400008bf8598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #27: _start + 0x30 (0x5f6e30 in /usr/bin/python3)

W0519 16:12:42.431000 164254 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006442_164254_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 117, in _call_store
    return getattr(self._store, store_op)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistNetworkError: failed to recv, got 0 bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 922, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 683, in _initialize_workers
    self._rendezvous(worker_group)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 500, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 1162, in next_rendezvous
    self._op_executor.run(join_op, deadline, self._get_deadline)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 648, in run
    has_set = self._state_holder.sync()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 437, in sync
    get_response = self._backend.get_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 75, in get_state
    base64_state: bytes = self._call_store("get", self._key)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 119, in _call_store
    raise RendezvousConnectionError(
torch.distributed.elastic.rendezvous.api.RendezvousConnectionError: The connection to the C10d store has failed. See inner exception for details.
[W519 16:12:42.488186049 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006442]:42228, remote=[nid006442]:12355): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x400085edbdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40003c7389c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x40003c738fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x40003c73d124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x148 (0x40003c73ec48 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2c (0x40003c73f10c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x9c (0x40003c74061c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xfd9044 (0x400036d69044 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x63c060 (0x4000363cc060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: /usr/bin/python3() [0x503e14]
frame #10: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python3)
frame #11: /usr/bin/python3() [0x4c709c]
frame #12: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #13: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python3)
frame #14: /usr/bin/python3() [0x529450]
frame #15: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python3)
frame #16: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #17: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python3)
frame #18: /usr/bin/python3() [0x55f968]
frame #19: /usr/bin/python3() [0x503c0c]
frame #20: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python3)
frame #21: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python3)
frame #22: /usr/bin/python3() [0x68bad8]
frame #23: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python3)
frame #24: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python3)
frame #25: <unknown function> + 0x284c4 (0x4000352684c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: __libc_start_main + 0x98 (0x400035268598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #27: _start + 0x30 (0x5f6e30 in /usr/bin/python3)

W0519 16:12:42.474000 164251 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006442_164251_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
[W519 16:12:42.499013213 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006442]:42228, remote=[nid006442]:12355): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x400085edbdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40003c7389c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x40003c738fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x40003c73d124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::doWait(c10::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x148 (0x40003c73ec48 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::doGet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x2c (0x40003c73f10c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::TCPStore::get(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x9c (0x40003c74061c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #7: <unknown function> + 0xfd9044 (0x400036d69044 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x63c060 (0x4000363cc060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #9: /usr/bin/python3() [0x503e14]
frame #10: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python3)
frame #11: /usr/bin/python3() [0x4c709c]
frame #12: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #13: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python3)
frame #14: /usr/bin/python3() [0x529450]
frame #15: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python3)
frame #16: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python3)
frame #17: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python3)
frame #18: /usr/bin/python3() [0x55f968]
frame #19: /usr/bin/python3() [0x503c0c]
frame #20: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python3)
frame #21: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python3)
frame #22: /usr/bin/python3() [0x68bad8]
frame #23: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python3)
frame #24: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python3)
frame #25: <unknown function> + 0x284c4 (0x4000352684c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: __libc_start_main + 0x98 (0x400035268598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #27: _start + 0x30 (0x5f6e30 in /usr/bin/python3)

W0519 16:12:42.484000 164251 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006442_164251_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 117, in _call_store
    return getattr(self._store, store_op)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistNetworkError: failed to recv, got 0 bytes

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 922, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 683, in _initialize_workers
    self._rendezvous(worker_group)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 500, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 1162, in next_rendezvous
    self._op_executor.run(join_op, deadline, self._get_deadline)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 648, in run
    has_set = self._state_holder.sync()
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py", line 437, in sync
    get_response = self._backend.get_state()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 75, in get_state
    base64_state: bytes = self._call_store("get", self._key)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py", line 119, in _call_store
    raise RendezvousConnectionError(
torch.distributed.elastic.rendezvous.api.RendezvousConnectionError: The connection to the C10d store has failed. See inner exception for details.
srun: error: nid006444: task 7: Exited with exit code 1
srun: Terminating StepId=450279.0
slurmstepd: error: *** STEP 450279.0 ON nid006442 CANCELLED AT 2025-05-19T16:12:42 ***
srun: error: nid006442: task 0: Terminated
srun: error: nid006442: tasks 1,3: Exited with exit code 1
srun: error: nid006444: tasks 4-6: Terminated
srun: Force Terminated StepId=450279.0
END TIME: Mon May 19 16:12:43 CEST 2025
