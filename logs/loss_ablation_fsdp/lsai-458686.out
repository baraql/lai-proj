START TIME: Thu May 22 19:57:05 CEST 2025
Node IP: 172.28.33.40
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
2025-05-22 19:57:27,481 - root - INFO - Setting seed to 42
2025-05-22 19:57:27,481 - root - INFO - Setting seed to 42
2025-05-22 19:57:27,483 - root - INFO - Setting seed to 42
2025-05-22 19:57:27,483 - root - INFO - Setting seed to 42
2025-05-22 19:57:27,483 - root - INFO - [RANK 0 / 4] Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, scaling_factor=10, scaling_strategy=<ScalingStrategy.ALL: 'all'>, set_seed=42)
2025-05-22 19:57:27,483 - root - INFO - [RANK 0 / 4] world size: 4
2025-05-22 19:57:27,483 - root - INFO - [RANK 0 / 4] Setting up DataLoaders...
2025-05-22 19:57:30,723 - root - INFO - [RANK 0 / 4] Setting up Model...
2025-05-22 19:57:30,723 - root - INFO - [RANK 0 / 4] Loading a model with scale=10, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=2560, n_layers=80, n_heads=80, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
Total model parameters: 7329958400
Total model parameters: 7329958400
Total model parameters: 7329958400
Total model parameters: 7329958400
2025-05-22 19:58:02,264 - root - INFO - [RANK 0 / 4] Wrapping model with FSDP
2025-05-22 19:58:03,661 - root - INFO - [rank 3] local params: 1832489600
2025-05-22 19:58:03,677 - root - INFO - [RANK 0 / 4] The model is now: FullyShardedDataParallel
2025-05-22 19:58:03,678 - root - INFO - [rank 0] local params: 1832489600
2025-05-22 19:58:03,681 - root - INFO - [RANK 0 / 4] Starting training!
2025-05-22 19:58:03,959 - root - INFO - [rank 1] local params: 1832489600
2025-05-22 19:58:04,068 - root - INFO - [rank 2] local params: 1832489600
2025-05-22 19:58:10,356 - root - INFO - [RANK 0 / 4] Step: 1 | Loss: 11.93 | Tokens per second: 613.70 | Training tokens per second (%): 19.38 | MFU (%): 1.31 | TFLOPs: 12.93
2025-05-22 19:58:19,662 - root - INFO - [RANK 0 / 4] Step: 5 | Loss: 11.97 | Tokens per second: 1760.63 | Training tokens per second (%): 11.41 | MFU (%): 3.75 | TFLOPs: 37.08
2025-05-22 19:58:30,812 - root - INFO - [RANK 0 / 4] Step: 10 | Loss: 11.92 | Tokens per second: 1836.94 | Training tokens per second (%): 25.72 | MFU (%): 3.91 | TFLOPs: 38.69
2025-05-22 19:58:41,678 - root - INFO - [RANK 0 / 4] Step: 15 | Loss: 11.61 | Tokens per second: 1884.73 | Training tokens per second (%): 35.21 | MFU (%): 4.01 | TFLOPs: 39.69
2025-05-22 19:58:52,451 - root - INFO - [RANK 0 / 4] Step: 20 | Loss: 11.30 | Tokens per second: 1901.15 | Training tokens per second (%): 34.78 | MFU (%): 4.05 | TFLOPs: 40.04
2025-05-22 19:59:02,923 - root - INFO - [RANK 0 / 4] Step: 25 | Loss: 10.83 | Tokens per second: 1955.90 | Training tokens per second (%): 18.28 | MFU (%): 4.17 | TFLOPs: 41.19
2025-05-22 19:59:13,954 - root - INFO - [RANK 0 / 4] Step: 30 | Loss: 10.06 | Tokens per second: 1856.71 | Training tokens per second (%): 26.99 | MFU (%): 3.95 | TFLOPs: 39.10
2025-05-22 19:59:25,171 - root - INFO - [RANK 0 / 4] Step: 35 | Loss: 10.04 | Tokens per second: 1825.80 | Training tokens per second (%): 13.78 | MFU (%): 3.89 | TFLOPs: 38.45
2025-05-22 19:59:35,830 - root - INFO - [RANK 0 / 4] Step: 40 | Loss: 10.16 | Tokens per second: 1921.41 | Training tokens per second (%): 9.95 | MFU (%): 4.09 | TFLOPs: 40.47
2025-05-22 19:59:46,639 - root - INFO - [RANK 0 / 4] Step: 45 | Loss: 9.66 | Tokens per second: 1894.91 | Training tokens per second (%): 15.59 | MFU (%): 4.04 | TFLOPs: 39.91
2025-05-22 19:59:57,364 - root - INFO - [RANK 0 / 4] Step: 50 | Loss: 9.66 | Tokens per second: 1909.53 | Training tokens per second (%): 10.93 | MFU (%): 4.07 | TFLOPs: 40.22
2025-05-22 20:00:08,110 - root - INFO - [RANK 0 / 4] Step: 55 | Loss: 10.03 | Tokens per second: 1906.03 | Training tokens per second (%): 28.32 | MFU (%): 4.06 | TFLOPs: 40.14
2025-05-22 20:00:18,946 - root - INFO - [RANK 0 / 4] Step: 60 | Loss: 9.37 | Tokens per second: 1889.98 | Training tokens per second (%): 26.71 | MFU (%): 4.02 | TFLOPs: 39.81
2025-05-22 20:00:29,713 - root - INFO - [RANK 0 / 4] Step: 65 | Loss: 9.58 | Tokens per second: 1902.20 | Training tokens per second (%): 24.18 | MFU (%): 4.05 | TFLOPs: 40.06
2025-05-22 20:00:40,676 - root - INFO - [RANK 0 / 4] Step: 70 | Loss: 9.07 | Tokens per second: 1868.37 | Training tokens per second (%): 26.25 | MFU (%): 3.98 | TFLOPs: 39.35
2025-05-22 20:00:51,450 - root - INFO - [RANK 0 / 4] Step: 75 | Loss: 8.88 | Tokens per second: 1900.92 | Training tokens per second (%): 16.89 | MFU (%): 4.05 | TFLOPs: 40.04
2025-05-22 20:01:02,165 - root - INFO - [RANK 0 / 4] Step: 80 | Loss: 8.78 | Tokens per second: 1911.46 | Training tokens per second (%): 17.36 | MFU (%): 4.07 | TFLOPs: 40.26
2025-05-22 20:01:12,922 - root - INFO - [RANK 0 / 4] Step: 85 | Loss: 8.81 | Tokens per second: 1903.93 | Training tokens per second (%): 16.04 | MFU (%): 4.05 | TFLOPs: 40.10
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 458686.0 ON nid006648 CANCELLED AT 2025-05-22T20:01:16 ***
slurmstepd: error: *** JOB 458686 ON nid006648 CANCELLED AT 2025-05-22T20:01:16 ***
srun: forcing job termination
srun: got SIGCONT
W0522 20:01:16.512000 64390 torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0522 20:01:16.512000 107249 torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0522 20:01:16.513000 107249 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 107609 closing signal SIGTERM
W0522 20:01:16.513000 64390 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 64749 closing signal SIGTERM
W0522 20:01:16.514000 64390 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 64750 closing signal SIGTERM
W0522 20:01:16.514000 107249 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 107610 closing signal SIGTERM
