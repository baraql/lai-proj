START TIME: Thu May 22 20:22:55 CEST 2025
Node IP: 172.28.33.64
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
2025-05-22 20:23:17,958 - root - INFO - Setting seed to 42
2025-05-22 20:23:17,958 - root - INFO - Setting seed to 42
2025-05-22 20:23:17,958 - root - INFO - Setting seed to 42
2025-05-22 20:23:17,958 - root - INFO - Setting seed to 42
2025-05-22 20:23:17,958 - root - INFO - [RANK 0 / 4] Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, scaling_factor=12, scaling_strategy=<ScalingStrategy.ALL: 'all'>, set_seed=42)
2025-05-22 20:23:17,959 - root - INFO - [RANK 0 / 4] world size: 4
2025-05-22 20:23:17,959 - root - INFO - [RANK 0 / 4] Setting up DataLoaders...
2025-05-22 20:23:21,167 - root - INFO - [RANK 0 / 4] Setting up Model...
2025-05-22 20:23:21,167 - root - INFO - [RANK 0 / 4] Loading a model with scale=12, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=3072, n_layers=96, n_heads=96, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
Total model parameters: 12281515008
Total model parameters: 12281515008
2025-05-22 20:24:10,091 - root - INFO - [RANK 0 / 4] Wrapping model with FSDP
Total model parameters: 12281515008
2025-05-22 20:24:11,248 - root - INFO - [RANK 0 / 4] The model is now: FullyShardedDataParallel
2025-05-22 20:24:11,249 - root - INFO - [rank 0] local params: 3070378752
2025-05-22 20:24:11,253 - root - INFO - [RANK 0 / 4] Starting training!
2025-05-22 20:24:11,546 - root - INFO - [rank 3] local params: 3070378752
Total model parameters: 12281515008
2025-05-22 20:24:13,047 - root - INFO - [rank 2] local params: 3070378752
2025-05-22 20:24:14,818 - root - INFO - [rank 1] local params: 3070378752
2025-05-22 20:24:22,077 - root - INFO - [RANK 0 / 4] Step: 1 | Loss: 11.95 | Tokens per second: 378.41 | Training tokens per second (%): 19.38 | MFU (%): 1.26 | TFLOPs: 12.46
2025-05-22 20:24:36,204 - root - INFO - [RANK 0 / 4] Step: 5 | Loss: 11.93 | Tokens per second: 1159.80 | Training tokens per second (%): 11.41 | MFU (%): 3.86 | TFLOPs: 38.18
2025-05-22 20:24:53,710 - root - INFO - [RANK 0 / 4] Step: 10 | Loss: 11.85 | Tokens per second: 1169.97 | Training tokens per second (%): 25.72 | MFU (%): 3.89 | TFLOPs: 38.51
2025-05-22 20:25:11,194 - root - INFO - [RANK 0 / 4] Step: 15 | Loss: 11.43 | Tokens per second: 1171.37 | Training tokens per second (%): 35.21 | MFU (%): 3.90 | TFLOPs: 38.56
2025-05-22 20:25:28,448 - root - INFO - [RANK 0 / 4] Step: 20 | Loss: 11.03 | Tokens per second: 1186.98 | Training tokens per second (%): 34.78 | MFU (%): 3.95 | TFLOPs: 39.07
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 458732.0 ON nid006656 CANCELLED AT 2025-05-22T20:25:41 ***
slurmstepd: error: *** JOB 458732 ON nid006656 CANCELLED AT 2025-05-22T20:25:41 ***
srun: forcing job termination
srun: got SIGCONT
W0522 20:25:41.365000 231855 torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0522 20:25:41.365000 228849 torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0522 20:25:41.366000 231855 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 232240 closing signal SIGTERM
W0522 20:25:41.366000 228849 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 229215 closing signal SIGTERM
W0522 20:25:41.367000 231855 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 232241 closing signal SIGTERM
W0522 20:25:41.367000 228849 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 229216 closing signal SIGTERM
