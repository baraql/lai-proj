START TIME: Wed May 21 00:39:25 CEST 2025
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
usage: train_fsdp.py [-h] [--dataset DATASET]
                     [--tokenizer-name-or-path TOKENIZER_NAME_OR_PATH]
                     [--sequence-length SEQUENCE_LENGTH]
                     [--batch-size BATCH_SIZE] [--fused-optimizer]
                     [--learning-rate LEARNING_RATE]
                     [--lr-warmup-steps LR_WARMUP_STEPS]
                     [--training-steps TRAINING_STEPS]
                     [--logging-frequency LOGGING_FREQUENCY] [--profile]
                     [--profile-step-start PROFILE_STEP_START]
                     [--profile-step-end PROFILE_STEP_END]
                     [--grad-max-norm GRAD_MAX_NORM]
                     [--model-dtype MODEL_DTYPE] [--compile]
                     [--scaling-factor SCALING_FACTOR]
                     [--scaling-strategy {ScalingStrategy.ALL,ScalingStrategy.N_LAYERS}]
                     [--set-seed SET_SEED]
train_fsdp.py: error: unrecognized arguments: --scaling_factor 19 --scaling_strategy all
E0521 00:40:20.616000 142701 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 143138) of binary: /usr/bin/python
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 922, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/iopsstor/scratch/cscs/elyulina/lai-proj/train_fsdp.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-05-21_00:40:20
  host      : nid006440
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 143139)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-05-21_00:40:20
  host      : nid006440
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 143140)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-05-21_00:40:20
  host      : nid006440
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 143141)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-21_00:40:20
  host      : nid006440
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 143138)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0521 00:40:21.117000 201926 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 202465) of binary: /usr/bin/python
[W521 00:40:21.889277426 TCPStore.cpp:115] [c10d] recvVector failed on SocketImpl(fd=3, addr=[nid006445-hsn3]:51850, remote=[nid006440]:29500): failed to recv, got 0 bytes
Exception raised from recvBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:671 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x400063adbdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40001a3389c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8d14 (0x40001a338d14 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ab208 (0x40001a33b208 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: <unknown function> + 0x58aca84 (0x40001a33ca84 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::TCPStore::compareSet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&) + 0x1bc (0x40001a33e9ac in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0xfd9460 (0x400014969460 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x63c060 (0x400013fcc060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #8: /usr/bin/python() [0x503e14]
frame #9: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python)
frame #10: /usr/bin/python() [0x4c709c]
frame #11: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python)
frame #12: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python)
frame #13: /usr/bin/python() [0x529450]
frame #14: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python)
frame #15: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python)
frame #16: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python)
frame #17: /usr/bin/python() [0x55f968]
frame #18: /usr/bin/python() [0x503c0c]
frame #19: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python)
frame #20: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python)
frame #21: /usr/bin/python() [0x68bad8]
frame #22: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python)
frame #23: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python)
frame #24: <unknown function> + 0x284c4 (0x400012e684c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #25: __libc_start_main + 0x98 (0x400012e68598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #26: _start + 0x30 (0x5f6e30 in /usr/bin/python)

W0521 00:40:21.132000 201926 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006445_201926_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
[W521 00:40:21.913978480 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006445-hsn3]:51850, remote=[nid006440]:29500): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x400063adbdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40001a3389c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x40001a338fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x40001a33d124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::compareSet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&) + 0x1ac (0x40001a33e99c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: <unknown function> + 0xfd9460 (0x400014969460 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #6: <unknown function> + 0x63c060 (0x400013fcc060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #7: /usr/bin/python() [0x503e14]
frame #8: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python)
frame #9: /usr/bin/python() [0x4c709c]
frame #10: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python)
frame #11: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python)
frame #12: /usr/bin/python() [0x529450]
frame #13: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python)
frame #14: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python)
frame #15: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python)
frame #16: /usr/bin/python() [0x55f968]
frame #17: /usr/bin/python() [0x503c0c]
frame #18: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python)
frame #19: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python)
frame #20: /usr/bin/python() [0x68bad8]
frame #21: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python)
frame #22: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python)
frame #23: <unknown function> + 0x284c4 (0x400012e684c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #24: __libc_start_main + 0x98 (0x400012e68598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #25: _start + 0x30 (0x5f6e30 in /usr/bin/python)

W0521 00:40:21.156000 201926 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006445_201926_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
[W521 00:40:21.935599307 TCPStore.cpp:106] [c10d] sendBytes failed on SocketImpl(fd=3, addr=[nid006445-hsn3]:51850, remote=[nid006440]:29500): Broken pipe
Exception raised from sendBytes at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/Utils.hpp:646 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0xd4 (0x400063adbdd4 in /usr/local/lib/python3.12/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x58a89c0 (0x40001a3389c0 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x58a8fcc (0x40001a338fcc in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x58ad124 (0x40001a33d124 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::compareSet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&, std::vector<unsigned char, std::allocator<unsigned char> > const&) + 0x1ac (0x40001a33e99c in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cpu.so)
frame #5: <unknown function> + 0xfd9460 (0x400014969460 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #6: <unknown function> + 0x63c060 (0x400013fcc060 in /usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_python.so)
frame #7: /usr/bin/python() [0x503e14]
frame #8: _PyObject_MakeTpCall + 0x78 (0x4c2db8 in /usr/bin/python)
frame #9: /usr/bin/python() [0x4c709c]
frame #10: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python)
frame #11: _PyObject_Call_Prepend + 0xc4 (0x4c4894 in /usr/bin/python)
frame #12: /usr/bin/python() [0x529450]
frame #13: PyObject_Call + 0xa4 (0x4c52d4 in /usr/bin/python)
frame #14: _PyEval_EvalFrameDefault + 0x3e70 (0x567d34 in /usr/bin/python)
frame #15: PyEval_EvalCode + 0x130 (0x562ab4 in /usr/bin/python)
frame #16: /usr/bin/python() [0x55f968]
frame #17: /usr/bin/python() [0x503c0c]
frame #18: PyObject_Vectorcall + 0x4c (0x4c396c in /usr/bin/python)
frame #19: _PyEval_EvalFrameDefault + 0x8a0 (0x564764 in /usr/bin/python)
frame #20: /usr/bin/python() [0x68bad8]
frame #21: Py_RunMain + 0x1ac (0x68b19c in /usr/bin/python)
frame #22: Py_BytesMain + 0x28 (0x68ae88 in /usr/bin/python)
frame #23: <unknown function> + 0x284c4 (0x400012e684c4 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #24: __libc_start_main + 0x98 (0x400012e68598 in /usr/lib/aarch64-linux-gnu/libc.so.6)
frame #25: _start + 0x30 (0x5f6e30 in /usr/bin/python)

W0521 00:40:21.177000 201926 torch/distributed/elastic/rendezvous/dynamic_rendezvous.py:1284] The node 'nid006445_201926_0' has failed to shutdown the rendezvous 'none' due to an error of type RendezvousConnectionError.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 922, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/iopsstor/scratch/cscs/elyulina/lai-proj/train_fsdp.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-05-21_00:40:21
  host      : nid006445
  rank      : 5 (local_rank: 1)
  exitcode  : 2 (pid: 202466)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-05-21_00:40:21
  host      : nid006445
  rank      : 6 (local_rank: 2)
  exitcode  : 2 (pid: 202467)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-05-21_00:40:21
  host      : nid006445
  rank      : 7 (local_rank: 3)
  exitcode  : 2 (pid: 202468)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-21_00:40:21
  host      : nid006445
  rank      : 4 (local_rank: 0)
  exitcode  : 2 (pid: 202465)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: nid006440: task 0: Exited with exit code 1
srun: Terminating StepId=454123.0
srun: error: nid006445: task 1: Exited with exit code 1
END TIME: Wed May 21 00:40:22 CEST 2025
