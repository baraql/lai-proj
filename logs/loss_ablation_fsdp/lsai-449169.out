START TIME: Mon May 19 13:19:22 CEST 2025
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
2025-05-19 13:19:43,126 - root - INFO - Setting seed to 42
2025-05-19 13:19:43,126 - root - INFO - Setting seed to 42
2025-05-19 13:19:43,126 - root - INFO - Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, scale=1, set_seed=42)
2025-05-19 13:19:43,126 - root - INFO - Setting seed to 42
2025-05-19 13:19:43,126 - root - INFO - Setting seed to 42
2025-05-19 13:19:50,066 - root - INFO - [rank 0] world size: 4
2025-05-19 13:19:50,066 - root - INFO - Setting up DataLoaders...
2025-05-19 13:19:53,095 - root - INFO - Setting up Model...
Total params: 8053329920
2025-05-19 13:20:26,598 - root - INFO - [rank 2] model is now: FullyShardedDataParallel
2025-05-19 13:20:26,599 - root - INFO - [rank 2] local params: 2013332480
Total params: 8053329920
2025-05-19 13:20:26,724 - root - INFO - [rank 1] model is now: FullyShardedDataParallel
2025-05-19 13:20:26,724 - root - INFO - [rank 1] local params: 2013332480
Total params: 8053329920
2025-05-19 13:20:26,892 - root - INFO - [rank 3] model is now: FullyShardedDataParallel
2025-05-19 13:20:26,893 - root - INFO - [rank 3] local params: 2013332480
Total params: 8053329920
2025-05-19 13:20:28,061 - root - INFO - [rank 0] model is now: FullyShardedDataParallel
2025-05-19 13:20:28,061 - root - INFO - [rank 0] local params: 2013332480
2025-05-19 13:20:28,063 - root - INFO - Starting training!
2025-05-19 13:20:38,144 - root - INFO - Step: 1 | Loss: 11.91 | Tokens per second: 406.62 | Training tokens per second (%): 19.38 | MFU (%): 0.76 | TFLOPs: 7.53
2025-05-19 13:20:40,407 - root - INFO - Step: 5 | Loss: 11.92 | Tokens per second: 7307.52 | Training tokens per second (%): 11.41 | MFU (%): 13.69 | TFLOPs: 135.35
2025-05-19 13:20:43,294 - root - INFO - Step: 10 | Loss: 11.89 | Tokens per second: 7144.77 | Training tokens per second (%): 25.72 | MFU (%): 13.38 | TFLOPs: 132.34
2025-05-19 13:20:46,214 - root - INFO - Step: 15 | Loss: 11.67 | Tokens per second: 7065.07 | Training tokens per second (%): 35.21 | MFU (%): 13.23 | TFLOPs: 130.86
2025-05-19 13:20:49,126 - root - INFO - Step: 20 | Loss: 11.32 | Tokens per second: 7084.54 | Training tokens per second (%): 34.78 | MFU (%): 13.27 | TFLOPs: 131.22
2025-05-19 13:20:51,999 - root - INFO - Step: 25 | Loss: 10.83 | Tokens per second: 7180.30 | Training tokens per second (%): 18.28 | MFU (%): 13.45 | TFLOPs: 133.00
2025-05-19 13:20:54,903 - root - INFO - Step: 30 | Loss: 9.78 | Tokens per second: 7103.38 | Training tokens per second (%): 26.99 | MFU (%): 13.30 | TFLOPs: 131.57
2025-05-19 13:20:57,753 - root - INFO - Step: 35 | Loss: 9.88 | Tokens per second: 7239.38 | Training tokens per second (%): 13.78 | MFU (%): 13.56 | TFLOPs: 134.09
2025-05-19 13:21:00,589 - root - INFO - Step: 40 | Loss: 9.86 | Tokens per second: 7276.81 | Training tokens per second (%): 9.95 | MFU (%): 13.63 | TFLOPs: 134.78
2025-05-19 13:21:03,453 - root - INFO - Step: 45 | Loss: 9.35 | Tokens per second: 7203.38 | Training tokens per second (%): 15.59 | MFU (%): 13.49 | TFLOPs: 133.42
2025-05-19 13:21:06,296 - root - INFO - Step: 50 | Loss: 9.22 | Tokens per second: 7256.09 | Training tokens per second (%): 10.93 | MFU (%): 13.59 | TFLOPs: 134.40
2025-05-19 13:21:09,206 - root - INFO - Step: 55 | Loss: 9.35 | Tokens per second: 7089.94 | Training tokens per second (%): 28.32 | MFU (%): 13.28 | TFLOPs: 131.32
2025-05-19 13:21:12,111 - root - INFO - Step: 60 | Loss: 8.70 | Tokens per second: 7100.64 | Training tokens per second (%): 26.71 | MFU (%): 13.30 | TFLOPs: 131.52
2025-05-19 13:21:15,005 - root - INFO - Step: 65 | Loss: 8.95 | Tokens per second: 7126.57 | Training tokens per second (%): 24.18 | MFU (%): 13.35 | TFLOPs: 132.00
2025-05-19 13:21:17,902 - root - INFO - Step: 70 | Loss: 8.24 | Tokens per second: 7122.28 | Training tokens per second (%): 26.25 | MFU (%): 13.34 | TFLOPs: 131.92
2025-05-19 13:21:20,771 - root - INFO - Step: 75 | Loss: 8.34 | Tokens per second: 7189.88 | Training tokens per second (%): 16.89 | MFU (%): 13.47 | TFLOPs: 133.17
2025-05-19 13:21:23,641 - root - INFO - Step: 80 | Loss: 8.24 | Tokens per second: 7188.25 | Training tokens per second (%): 17.36 | MFU (%): 13.46 | TFLOPs: 133.14
2025-05-19 13:21:26,504 - root - INFO - Step: 85 | Loss: 8.34 | Tokens per second: 7206.17 | Training tokens per second (%): 16.04 | MFU (%): 13.50 | TFLOPs: 133.48
2025-05-19 13:21:29,528 - root - INFO - Step: 90 | Loss: 7.80 | Tokens per second: 6820.89 | Training tokens per second (%): 57.98 | MFU (%): 12.77 | TFLOPs: 126.34
2025-05-19 13:21:32,539 - root - INFO - Step: 95 | Loss: 7.53 | Tokens per second: 6848.33 | Training tokens per second (%): 57.90 | MFU (%): 12.83 | TFLOPs: 126.85
2025-05-19 13:21:35,666 - root - INFO - Step: 100 | Loss: 7.48 | Tokens per second: 6593.91 | Training tokens per second (%): 93.89 | MFU (%): 12.35 | TFLOPs: 122.14
2025-05-19 13:21:38,542 - root - INFO - Step: 105 | Loss: 7.75 | Tokens per second: 7173.94 | Training tokens per second (%): 17.91 | MFU (%): 13.44 | TFLOPs: 132.88
2025-05-19 13:21:41,432 - root - INFO - Step: 110 | Loss: 8.32 | Tokens per second: 7137.62 | Training tokens per second (%): 25.76 | MFU (%): 13.37 | TFLOPs: 132.21
2025-05-19 13:21:44,271 - root - INFO - Step: 115 | Loss: 7.72 | Tokens per second: 7266.16 | Training tokens per second (%): 9.98 | MFU (%): 13.61 | TFLOPs: 134.59
2025-05-19 13:21:47,183 - root - INFO - Step: 120 | Loss: 7.91 | Tokens per second: 7085.55 | Training tokens per second (%): 26.43 | MFU (%): 13.27 | TFLOPs: 131.24
2025-05-19 13:21:50,072 - root - INFO - Step: 125 | Loss: 8.18 | Tokens per second: 7139.52 | Training tokens per second (%): 24.73 | MFU (%): 13.37 | TFLOPs: 132.24
2025-05-19 13:21:52,946 - root - INFO - Step: 130 | Loss: 8.68 | Tokens per second: 7179.22 | Training tokens per second (%): 17.81 | MFU (%): 13.45 | TFLOPs: 132.98
2025-05-19 13:21:55,828 - root - INFO - Step: 135 | Loss: 7.72 | Tokens per second: 7158.90 | Training tokens per second (%): 21.95 | MFU (%): 13.41 | TFLOPs: 132.60
2025-05-19 13:21:58,676 - root - INFO - Step: 140 | Loss: 7.38 | Tokens per second: 7243.32 | Training tokens per second (%): 11.89 | MFU (%): 13.57 | TFLOPs: 134.16
2025-05-19 13:22:01,587 - root - INFO - Step: 145 | Loss: 7.56 | Tokens per second: 7085.15 | Training tokens per second (%): 26.10 | MFU (%): 13.27 | TFLOPs: 131.23
2025-05-19 13:22:04,465 - root - INFO - Step: 150 | Loss: 7.84 | Tokens per second: 7169.11 | Training tokens per second (%): 19.74 | MFU (%): 13.43 | TFLOPs: 132.79
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 449169.0 ON nid006445 CANCELLED AT 2025-05-19T13:22:05 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 449169 ON nid006445 CANCELLED AT 2025-05-19T13:22:05 DUE TO TIME LIMIT ***
W0519 13:22:06.015000 148606 torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0519 13:22:06.016000 148606 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 149179 closing signal SIGTERM
W0519 13:22:06.020000 148606 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 149180 closing signal SIGTERM
srun: forcing job termination
srun: got SIGCONT
W0519 13:22:06.024000 148606 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 149179 closing signal SIGTERM
W0519 13:22:06.024000 148606 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 149180 closing signal SIGTERM
W0519 13:22:06.025000 148606 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 149181 closing signal SIGTERM
W0519 13:22:06.027000 148606 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 149182 closing signal SIGTERM
