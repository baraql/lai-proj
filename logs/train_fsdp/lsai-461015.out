START TIME: Fri May 23 18:53:52 CEST 2025
Node IP: 172.28.53.27
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
2025-05-23 18:54:15,063 - root - INFO - [RANK 0 / 16] Setting seed to 42
2025-05-23 18:54:15,063 - root - INFO - [RANK 0 / 16] AVAILABLE GPUS: 16
2025-05-23 18:54:15,063 - root - INFO - [RANK 0 / 16] NODES: 4.0
2025-05-23 18:54:15,063 - root - INFO - [RANK 0 / 16] Total RAM: 854.46 GB
2025-05-23 18:54:15,064 - root - INFO - [RANK 0 / 16] Available RAM: 775.84 GB
2025-05-23 18:54:15,064 - root - INFO - [RANK 0 / 16] Available per-process RAM: 193.96 GB
2025-05-23 18:54:21,751 - root - INFO - [RANK 0 / 16] GPU 0: NVIDIA GH200 120GB
2025-05-23 18:54:21,751 - root - INFO - [RANK 0 / 16]   Total memory: 94.50 GB
2025-05-23 18:54:21,751 - root - INFO - [RANK 0 / 16]   Allocated memory: 0.00 GB
2025-05-23 18:54:21,751 - root - INFO - [RANK 0 / 16]   Cached memory: 0.00 GB
2025-05-23 18:54:21,752 - root - INFO - [RANK 0 / 16] Experiment args: Namespace(dataset='/capstor/store/cscs/ethz/large-sc/datasets/train_data.parquet', tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=4096, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=100, logging_frequency=5, profile=False, profile_step_start=10, profile_step_end=12, grad_max_norm=1, model_dtype='bf16', compile=False, scaling_factor=14, scaling_strategy=<ScalingStrategy.ALL: 'all'>, set_seed=42)
2025-05-23 18:54:21,752 - root - INFO - [RANK 0 / 16] world size: 16
2025-05-23 18:54:21,752 - root - INFO - [RANK 0 / 16] Setting up DataLoaders...
2025-05-23 18:54:24,848 - root - INFO - [RANK 0 / 16] Setting up Model...
2025-05-23 18:54:24,848 - root - INFO - [RANK 0 / 16] Loading a model with scale=14, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=3584, n_layers=112, n_heads=112, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
Total model parameters: 19128929792
2025-05-23 18:55:37,568 - root - INFO - [RANK 0 / 16] Wrapping model with FSDP
Total model parameters: 19128929792
Total model parameters: 19128929792
2025-05-23 18:55:38,261 - root - INFO - [rank 11] local params: 1195558112
2025-05-23 18:55:38,261 - root - INFO - [rank 10] local params: 1195558112
2025-05-23 18:55:38,322 - root - INFO - [rank 8] local params: 1195558112
Total model parameters: 19128929792
2025-05-23 18:55:38,607 - root - INFO - [rank 1] local params: 1195558112
Total model parameters: 19128929792
Total model parameters: 19128929792
2025-05-23 18:55:38,729 - root - INFO - [rank 14] local params: 1195558112
2025-05-23 18:55:38,794 - root - INFO - [rank 15] local params: 1195558112
2025-05-23 18:55:38,816 - root - INFO - [RANK 0 / 16] The model is now: FullyShardedDataParallel
2025-05-23 18:55:38,817 - root - INFO - [rank 0] local params: 1195558112
2025-05-23 18:55:38,818 - root - INFO - [rank 2] local params: 1195558112
2025-05-23 18:55:38,823 - root - INFO - [RANK 0 / 16] Starting training!
Total model parameters: 19128929792
2025-05-23 18:55:38,904 - root - INFO - [rank 4] local params: 1195558112
2025-05-23 18:55:39,102 - root - INFO - [rank 7] local params: 1195558112
Total model parameters: 19128929792
2025-05-23 18:55:39,214 - root - INFO - [rank 13] local params: 1195558112
2025-05-23 18:55:39,768 - root - INFO - [rank 9] local params: 1195558112
2025-05-23 18:55:40,022 - root - INFO - [rank 6] local params: 1195558112
2025-05-23 18:55:40,223 - root - INFO - [rank 5] local params: 1195558112
2025-05-23 18:55:40,236 - root - INFO - [rank 12] local params: 1195558112
2025-05-23 18:55:40,264 - root - INFO - [rank 3] local params: 1195558112
2025-05-23 18:55:51,927 - root - INFO - [RANK 0 / 16] Step: 1 | Loss: 11.92 | Tokens per second: 312.57 | Training tokens per second (%): 19.38 | MFU (%): 0.85 | TFLOPs: 8.41
2025-05-23 18:56:12,896 - root - INFO - [RANK 0 / 16] Step: 5 | Loss: 12.00 | Tokens per second: 781.37 | Training tokens per second (%): 11.41 | MFU (%): 2.13 | TFLOPs: 21.02
2025-05-23 18:56:39,389 - root - INFO - [RANK 0 / 16] Step: 10 | Loss: 11.79 | Tokens per second: 773.04 | Training tokens per second (%): 25.72 | MFU (%): 2.10 | TFLOPs: 20.80
2025-05-23 18:57:04,302 - root - INFO - [RANK 0 / 16] Step: 15 | Loss: 11.28 | Tokens per second: 822.10 | Training tokens per second (%): 35.21 | MFU (%): 2.24 | TFLOPs: 22.12
2025-05-23 18:57:28,776 - root - INFO - [RANK 0 / 16] Step: 20 | Loss: 10.72 | Tokens per second: 836.85 | Training tokens per second (%): 34.78 | MFU (%): 2.28 | TFLOPs: 22.51
2025-05-23 18:57:52,429 - root - INFO - [RANK 0 / 16] Step: 25 | Loss: 10.35 | Tokens per second: 865.84 | Training tokens per second (%): 18.28 | MFU (%): 2.36 | TFLOPs: 23.29
2025-05-23 18:58:14,790 - root - INFO - [RANK 0 / 16] Step: 30 | Loss: 9.55 | Tokens per second: 915.90 | Training tokens per second (%): 26.99 | MFU (%): 2.49 | TFLOPs: 24.64
2025-05-23 18:58:35,962 - root - INFO - [RANK 0 / 16] Step: 35 | Loss: 9.65 | Tokens per second: 967.33 | Training tokens per second (%): 13.78 | MFU (%): 2.63 | TFLOPs: 26.02
2025-05-23 18:58:57,254 - root - INFO - [RANK 0 / 16] Step: 40 | Loss: 9.82 | Tokens per second: 961.92 | Training tokens per second (%): 9.95 | MFU (%): 2.62 | TFLOPs: 25.88
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 461015.0 ON nid007112 CANCELLED AT 2025-05-23T18:59:01 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 461015 ON nid007112 CANCELLED AT 2025-05-23T18:59:01 DUE TO TIME LIMIT ***
W0523 18:59:01.017000 109628 torch/distributed/elastic/agent/server/api.py:719] Received 15 death signal, shutting down workers
W0523 18:59:01.018000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109972 closing signal SIGTERM
srun: forcing job termination
srun: got SIGCONT
W0523 18:59:01.019000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109973 closing signal SIGTERM
W0523 18:59:01.020000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109974 closing signal SIGTERM
W0523 18:59:01.021000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109975 closing signal SIGTERM
--- Logging error ---
--- Logging error ---
--- Logging error ---
W0523 18:59:01.029000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109972 closing signal SIGTERM
W0523 18:59:01.030000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109973 closing signal SIGTERM
W0523 18:59:01.030000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109974 closing signal SIGTERM
W0523 18:59:01.030000 109628 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 109975 closing signal SIGTERM
