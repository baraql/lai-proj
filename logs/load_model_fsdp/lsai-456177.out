START TIME: Wed May 21 23:20:20 CEST 2025
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
2025-05-21 23:20:44,986 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:44,986 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:44,988 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:44,988 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:45,010 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:45,011 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:45,011 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:20:45,012 - root - INFO - Loading a model with scale=19, scaling_strategy=ScalingStrategy.ALL, config:
TransformerModelArgs(dim=4864, n_layers=152, n_heads=152, n_kv_heads=8, multiple_of=256, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, norm_type='rmsnorm', seq_len=4096, vocab_size=131072)
2025-05-21 23:23:33,232 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:33,274 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:34,050 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:34,304 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:35,319 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:35,933 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:36,102 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:37,495 - root - INFO - Total model parameters: 46,322,328,320
2025-05-21 23:23:42,188 - root - INFO - [rank 0]Took 2 min 57 sec
2025-05-21 23:23:42,189 - root - INFO - [rank 0]


END TIME: Wed May 21 23:23:50 CEST 2025

WORKED WITH THIS SETUP

#!/bin/bash

#SBATCH --account=a-large-sc
#SBATCH --partition=normal
#SBATCH --time=00:29:59
#SBATCH --job-name=lsai
#SBATCH --output=/iopsstor/scratch/cscs/$MY_USER/lai-proj/logs/load_model_fsdp/%x-%j.out
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=4
#SBATCH --cpus-per-task=72
#SBATCH --mem=460000
#SBATCH --environment=/iopsstor/scratch/cscs/$MY_USER/ngc_pt_jan.toml     # Vanilla 25.01 PyTorch NGC Image 
#SBATCH --no-requeue	# Prevent Slurm to requeue the job if the execution crashes (e.g. node failure) so we don't loose the logs

set -eo pipefail

echo "START TIME: $(date)"

# Set up ENV
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
ASSIGNMENT_DIR="/iopsstor/scratch/cscs/$MY_USER/lai-proj"

CMD_PREFIX="numactl --membind=0-3"

export MASTER_ADDR=$(scontrol show hostnames $SLURM_NODELIST | head -n 1)
export MASTER_PORT=29500                     # any free port is fine
export WORLD_SIZE=$(($SLURM_GPUS_PER_NODE * $SLURM_JOB_NUM_NODES))
export RANK=$SLURM_PROCID                    # unique rank per process

srun python -m torch.distributed.run \
    --nnodes=$SLURM_JOB_NUM_NODES \
    --nproc_per_node=$SLURM_GPUS_PER_NODE \
    --node_rank=$SLURM_NODEID \
    --rdzv_backend=c10d \
    --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
    $ASSIGNMENT_DIR/load_model_fsdp.py 

echo "END TIME: $(date)"
